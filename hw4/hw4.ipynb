{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tdj1XLuceOk-"
   },
   "source": [
    "# Week 03: Word Representation\n",
    "The assignment this week is to distinguish between good and bad phrases of the word \"**earn**\" (e.g., earn money). You will practice using word2vector,  one of the methods learned today, in the process. \n",
    "\n",
    "Data used in this assignment:  \n",
    "https://drive.google.com/drive/folders/1qTIrefo4EFbsVF3LXhKbiahbIrvCLUBJ?usp=sharing\n",
    "\n",
    "* train.tsv: Some phrases with labels to train and validate the classification model. There are only two types of label: 1 means *good*; 0 means *bad*.\n",
    "* test.tsv: Same format as train.tsv. It's used to test your model.\n",
    "* GoogleNews-vectors-negative300.bin.gz: a pre-trained word2vector model trained by Google ([source](https://code.google.com/archive/p/word2vec/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GzvI76xeOlH"
   },
   "source": [
    "## Requirements\n",
    "* pandas\n",
    "* tensorflow\n",
    "* sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jk5Xag5ueOlI"
   },
   "source": [
    "## Read Data\n",
    "We use dataframe to store data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UYsjz2eCeOlI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         phrase  class\n",
      "0      earn a strong reputation      1\n",
      "1  Marty will surely earn every      0\n",
      "2             to earn between $      0\n",
      "3          to earn some college      0\n",
      "4        that earn rave reviews      0\n",
      "                   phrase  class\n",
      "0  degree earn 62 percent      0\n",
      "1     earn maybe 30 or 50      0\n",
      "2  earn the kind of money      1\n",
      "3      earn his 14th save      1\n",
      "4   earn a smaller amount      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def loadData(path):\n",
    "    ngram = []\n",
    "    _class = []\n",
    "    with open(path) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip(\"\\n\").split(\"\\t\")\n",
    "            ngram.append(line[0])\n",
    "            _class.append(int(line[1]))\n",
    "    return pd.DataFrame({\"phrase\":ngram,\"class\":_class})\n",
    "train = loadData(\"train.tsv\")\n",
    "print(train.head())\n",
    "test = loadData(\"test.tsv\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sl8pGQx2eOlL"
   },
   "source": [
    "## load word2vec model\n",
    "<font color=\"red\">**[ TODO ]**</font> Please load [GoogleNews-vectors-negative300.bin.gz](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g) model and check the embedding of the word `language`.\n",
    "\n",
    "* package `gensim` is a good choice (Look up the documentation [here](https://radimrehurek.com/gensim/models/word2vec.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DQjCDqZyeOlO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.30712891e-02  1.68457031e-02  1.54296875e-01  1.27929688e-01\n",
      " -2.67578125e-01  3.51562500e-02  1.19140625e-01  2.48046875e-01\n",
      "  1.93359375e-01 -7.95898438e-02  1.46484375e-01 -1.43554688e-01\n",
      " -3.04687500e-01  3.46679688e-02 -1.85546875e-02  1.06933594e-01\n",
      " -1.52343750e-01  2.89062500e-01  2.35595703e-02 -3.80859375e-01\n",
      "  1.09863281e-01  4.41406250e-01  3.75976562e-02 -1.22680664e-02\n",
      "  1.62353516e-02 -2.24609375e-01  7.61718750e-02 -3.12500000e-02\n",
      " -2.16064453e-02  1.49414062e-01 -4.02832031e-02 -4.46777344e-02\n",
      " -1.72851562e-01  3.32031250e-02  1.50390625e-01 -5.05371094e-02\n",
      "  2.72216797e-02  3.00781250e-01 -1.33789062e-01 -7.56835938e-02\n",
      "  1.93359375e-01 -1.98242188e-01 -1.27563477e-02  4.19921875e-01\n",
      " -2.19726562e-01  1.44531250e-01 -3.93066406e-02  1.94335938e-01\n",
      " -3.12500000e-01  1.84570312e-01  1.48773193e-04 -1.67968750e-01\n",
      " -7.37304688e-02 -3.12500000e-02  1.57226562e-01  3.30078125e-01\n",
      " -1.42578125e-01 -3.16406250e-01 -7.32421875e-02 -5.76171875e-02\n",
      "  1.02050781e-01 -1.08886719e-01  1.24023438e-01 -2.50244141e-02\n",
      " -2.49023438e-01  1.25976562e-01 -1.79687500e-01  3.32031250e-01\n",
      "  7.14111328e-03  2.51953125e-01  4.34570312e-02 -4.34570312e-02\n",
      " -3.90625000e-01  1.76757812e-01 -1.13525391e-02 -1.97753906e-02\n",
      "  2.79296875e-01  2.36328125e-01  1.19140625e-01  5.59082031e-02\n",
      "  1.73828125e-01 -1.10839844e-01 -4.95605469e-02  2.13867188e-01\n",
      "  6.17675781e-02  1.38671875e-01 -4.45556641e-03  2.55859375e-01\n",
      "  1.80664062e-01  5.88378906e-02 -6.59179688e-02 -2.08007812e-01\n",
      " -1.19140625e-01 -1.57226562e-01  5.02929688e-02 -6.29882812e-02\n",
      "  5.00488281e-02 -7.27539062e-02  1.74560547e-02 -3.56445312e-02\n",
      " -1.93359375e-01  3.93066406e-02 -3.36914062e-02 -1.07421875e-01\n",
      "  5.78613281e-02 -8.20312500e-02  1.74560547e-02 -1.65039062e-01\n",
      "  1.46484375e-01 -3.08837891e-02 -3.86718750e-01  2.49023438e-01\n",
      "  8.74023438e-02 -2.15820312e-01 -4.10156250e-02  1.60156250e-01\n",
      "  1.85546875e-01 -2.27050781e-02 -3.73535156e-02  7.86132812e-02\n",
      " -1.46484375e-01  6.78710938e-02  1.26953125e-01  3.30078125e-01\n",
      "  1.11328125e-01  9.27734375e-02 -3.45703125e-01 -1.41601562e-01\n",
      " -5.29785156e-02 -1.50390625e-01 -7.81250000e-02 -1.27929688e-01\n",
      " -4.02343750e-01 -1.41601562e-01  8.44726562e-02  1.08398438e-01\n",
      " -4.44335938e-02  3.73535156e-02  5.61523438e-02 -1.91406250e-01\n",
      "  1.54296875e-01 -5.12695312e-02 -6.49414062e-02 -8.30078125e-02\n",
      "  7.17773438e-02 -1.33789062e-01  1.05468750e-01  3.33984375e-01\n",
      " -1.08398438e-01  1.91650391e-02  2.14843750e-01  2.15820312e-01\n",
      " -1.05468750e-01 -1.44531250e-01  4.32128906e-02 -2.71484375e-01\n",
      " -3.78906250e-01  1.09863281e-01 -8.15429688e-02 -6.12792969e-02\n",
      " -1.33789062e-01  9.71679688e-02 -1.04370117e-02 -1.21093750e-01\n",
      " -2.44140625e-01  1.02050781e-01  1.10839844e-01 -1.00585938e-01\n",
      "  1.71875000e-01 -3.61328125e-02 -4.39453125e-02  2.83203125e-01\n",
      " -8.93554688e-02 -1.70898438e-01  2.46093750e-01  1.16699219e-01\n",
      "  8.39843750e-02 -1.32812500e-01 -1.61132812e-01 -1.39648438e-01\n",
      " -8.59375000e-02 -1.37695312e-01 -9.32617188e-02 -1.33789062e-01\n",
      "  1.65039062e-01  4.93164062e-02 -1.21093750e-01 -2.11914062e-01\n",
      "  1.61132812e-01 -1.07421875e-01 -3.97949219e-02 -3.51562500e-01\n",
      " -5.02929688e-02  1.46484375e-01 -4.68750000e-02  4.17480469e-02\n",
      " -1.27929688e-01 -9.76562500e-02 -2.46093750e-01  6.78710938e-02\n",
      " -2.30468750e-01  1.80664062e-02  3.54003906e-02  7.32421875e-02\n",
      " -2.23632812e-01 -1.25976562e-01  2.12890625e-01 -3.93066406e-02\n",
      " -2.41699219e-02 -9.61914062e-02  7.51953125e-02 -1.46484375e-01\n",
      " -1.49414062e-01 -8.83789062e-02 -4.88281250e-02  2.32421875e-01\n",
      "  3.30078125e-01  1.59179688e-01 -2.35351562e-01 -1.25976562e-01\n",
      "  2.68554688e-02 -5.29785156e-02 -6.59179688e-02 -2.17773438e-01\n",
      " -6.37817383e-03 -2.53906250e-01  2.28515625e-01  4.93164062e-02\n",
      "  3.54003906e-02  1.66992188e-01 -7.27539062e-02 -2.53906250e-01\n",
      " -1.34765625e-01  3.69140625e-01  1.83593750e-01 -1.64062500e-01\n",
      "  2.26562500e-01 -8.88671875e-02  3.69140625e-01  5.54199219e-02\n",
      " -3.63769531e-02 -1.48437500e-01  9.13085938e-02  2.47955322e-04\n",
      "  2.67578125e-01 -1.63085938e-01  1.19628906e-01  2.77343750e-01\n",
      " -1.49414062e-01  1.33789062e-01 -8.25195312e-02 -1.74804688e-01\n",
      " -1.77734375e-01  2.06054688e-01  5.07812500e-02 -2.08007812e-01\n",
      " -1.74804688e-01  9.66796875e-02  6.98242188e-02 -5.79833984e-04\n",
      "  9.22851562e-02  7.95898438e-02  1.41601562e-01  8.72802734e-03\n",
      " -8.05664062e-02  4.80957031e-02  2.49023438e-01 -1.64062500e-01\n",
      " -4.66308594e-02 -2.81250000e-01 -1.66015625e-01 -2.22656250e-01\n",
      " -2.32421875e-01  1.32812500e-01  4.15039062e-02  1.15234375e-01\n",
      " -7.66601562e-02 -1.10839844e-01 -1.97265625e-01  3.06396484e-02\n",
      " -1.03515625e-01  2.49023438e-02 -2.52685547e-02  3.39355469e-02\n",
      "  4.29687500e-02 -1.44531250e-01  2.12402344e-02  2.28271484e-02\n",
      " -1.88476562e-01  3.22265625e-01 -1.13281250e-01 -7.61718750e-02\n",
      "  2.94921875e-01 -1.33789062e-01 -1.80664062e-02 -6.25610352e-03\n",
      " -1.62353516e-02  5.98144531e-02  1.21582031e-01  4.17480469e-02]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "print(w2v_model[\"language\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fap51QcAeOlO"
   },
   "source": [
    "<font color=\"green\">Expected output: </font>\n",
    "\n",
    ">  <font face='monospace' size=3>\\[&nbsp;2.30712891e-02&nbsp;&nbsp;1.68457031e-02&nbsp;&nbsp;1.54296875e-01&nbsp; 1.27929688e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.67578125e-01&nbsp;&nbsp;3.51562500e-02&nbsp;&nbsp;1.19140625e-01&nbsp; 2.48046875e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.93359375e-01&nbsp;-7.95898438e-02&nbsp;&nbsp;1.46484375e-01&nbsp;-1.43554688e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-3.04687500e-01&nbsp;&nbsp;3.46679688e-02&nbsp;-1.85546875e-02&nbsp; 1.06933594e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.52343750e-01&nbsp;&nbsp;2.89062500e-01&nbsp;&nbsp;2.35595703e-02&nbsp;-3.80859375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.09863281e-01&nbsp;&nbsp;4.41406250e-01&nbsp;&nbsp;3.75976562e-02&nbsp;-1.22680664e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.62353516e-02&nbsp;-2.24609375e-01&nbsp;&nbsp;7.61718750e-02&nbsp;-3.12500000e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.16064453e-02&nbsp;&nbsp;1.49414062e-01&nbsp;-4.02832031e-02&nbsp;-4.46777344e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.72851562e-01&nbsp;&nbsp;3.32031250e-02&nbsp;&nbsp;1.50390625e-01&nbsp;-5.05371094e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.72216797e-02&nbsp;&nbsp;3.00781250e-01&nbsp;-1.33789062e-01&nbsp;-7.56835938e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.93359375e-01&nbsp;-1.98242188e-01&nbsp;-1.27563477e-02&nbsp; 4.19921875e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.19726562e-01&nbsp;&nbsp;1.44531250e-01&nbsp;-3.93066406e-02&nbsp; 1.94335938e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-3.12500000e-01&nbsp;&nbsp;1.84570312e-01&nbsp;&nbsp;1.48773193e-04&nbsp;-1.67968750e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-7.37304688e-02&nbsp;-3.12500000e-02&nbsp;&nbsp;1.57226562e-01&nbsp; 3.30078125e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.42578125e-01&nbsp;-3.16406250e-01&nbsp;-7.32421875e-02&nbsp;-5.76171875e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.02050781e-01&nbsp;-1.08886719e-01&nbsp;&nbsp;1.24023438e-01&nbsp;-2.50244141e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.49023438e-01&nbsp;&nbsp;1.25976562e-01&nbsp;-1.79687500e-01&nbsp; 3.32031250e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;7.14111328e-03&nbsp;&nbsp;2.51953125e-01&nbsp;&nbsp;4.34570312e-02&nbsp;-4.34570312e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-3.90625000e-01&nbsp;&nbsp;1.76757812e-01&nbsp;-1.13525391e-02&nbsp;-1.97753906e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.79296875e-01&nbsp;&nbsp;2.36328125e-01&nbsp;&nbsp;1.19140625e-01&nbsp; 5.59082031e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.73828125e-01&nbsp;-1.10839844e-01&nbsp;-4.95605469e-02&nbsp; 2.13867188e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;6.17675781e-02&nbsp;&nbsp;1.38671875e-01&nbsp;-4.45556641e-03&nbsp; 2.55859375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.80664062e-01&nbsp;&nbsp;5.88378906e-02&nbsp;-6.59179688e-02&nbsp;-2.08007812e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.19140625e-01&nbsp;-1.57226562e-01&nbsp;&nbsp;5.02929688e-02&nbsp;-6.29882812e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;5.00488281e-02&nbsp;-7.27539062e-02&nbsp;&nbsp;1.74560547e-02&nbsp;-3.56445312e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.93359375e-01&nbsp;&nbsp;3.93066406e-02&nbsp;-3.36914062e-02&nbsp;-1.07421875e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;5.78613281e-02&nbsp;-8.20312500e-02&nbsp;&nbsp;1.74560547e-02&nbsp;-1.65039062e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.46484375e-01&nbsp;-3.08837891e-02&nbsp;-3.86718750e-01&nbsp; 2.49023438e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;8.74023438e-02&nbsp;-2.15820312e-01&nbsp;-4.10156250e-02&nbsp; 1.60156250e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.85546875e-01&nbsp;-2.27050781e-02&nbsp;-3.73535156e-02&nbsp; 7.86132812e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.46484375e-01&nbsp;&nbsp;6.78710938e-02&nbsp;&nbsp;1.26953125e-01&nbsp; 3.30078125e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.11328125e-01&nbsp;&nbsp;9.27734375e-02&nbsp;-3.45703125e-01&nbsp;-1.41601562e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-5.29785156e-02&nbsp;-1.50390625e-01&nbsp;-7.81250000e-02&nbsp;-1.27929688e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-4.02343750e-01&nbsp;-1.41601562e-01&nbsp;&nbsp;8.44726562e-02&nbsp; 1.08398438e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-4.44335938e-02&nbsp;&nbsp;3.73535156e-02&nbsp;&nbsp;5.61523438e-02&nbsp;-1.91406250e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.54296875e-01&nbsp;-5.12695312e-02&nbsp;-6.49414062e-02&nbsp;-8.30078125e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;7.17773438e-02&nbsp;-1.33789062e-01&nbsp;&nbsp;1.05468750e-01&nbsp; 3.33984375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.08398438e-01&nbsp;&nbsp;1.91650391e-02&nbsp;&nbsp;2.14843750e-01&nbsp; 2.15820312e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.05468750e-01&nbsp;-1.44531250e-01&nbsp;&nbsp;4.32128906e-02&nbsp;-2.71484375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-3.78906250e-01&nbsp;&nbsp;1.09863281e-01&nbsp;-8.15429688e-02&nbsp;-6.12792969e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.33789062e-01&nbsp;&nbsp;9.71679688e-02&nbsp;-1.04370117e-02&nbsp;-1.21093750e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.44140625e-01&nbsp;&nbsp;1.02050781e-01&nbsp;&nbsp;1.10839844e-01&nbsp;-1.00585938e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.71875000e-01&nbsp;-3.61328125e-02&nbsp;-4.39453125e-02&nbsp; 2.83203125e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-8.93554688e-02&nbsp;-1.70898438e-01&nbsp;&nbsp;2.46093750e-01&nbsp; 1.16699219e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;8.39843750e-02&nbsp;-1.32812500e-01&nbsp;-1.61132812e-01&nbsp;-1.39648438e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-8.59375000e-02&nbsp;-1.37695312e-01&nbsp;-9.32617188e-02&nbsp;-1.33789062e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.65039062e-01&nbsp;&nbsp;4.93164062e-02&nbsp;-1.21093750e-01&nbsp;-2.11914062e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;1.61132812e-01&nbsp;-1.07421875e-01&nbsp;-3.97949219e-02&nbsp;-3.51562500e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-5.02929688e-02&nbsp;&nbsp;1.46484375e-01&nbsp;-4.68750000e-02&nbsp; 4.17480469e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.27929688e-01&nbsp;-9.76562500e-02&nbsp;-2.46093750e-01&nbsp; 6.78710938e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.30468750e-01&nbsp;&nbsp;1.80664062e-02&nbsp;&nbsp;3.54003906e-02&nbsp; 7.32421875e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.23632812e-01&nbsp;-1.25976562e-01&nbsp;&nbsp;2.12890625e-01&nbsp;-3.93066406e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.41699219e-02&nbsp;-9.61914062e-02&nbsp;&nbsp;7.51953125e-02&nbsp;-1.46484375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.49414062e-01&nbsp;-8.83789062e-02&nbsp;-4.88281250e-02&nbsp; 2.32421875e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;3.30078125e-01&nbsp;&nbsp;1.59179688e-01&nbsp;-2.35351562e-01&nbsp;-1.25976562e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.68554688e-02&nbsp;-5.29785156e-02&nbsp;-6.59179688e-02&nbsp;-2.17773438e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-6.37817383e-03&nbsp;-2.53906250e-01&nbsp;&nbsp;2.28515625e-01&nbsp; 4.93164062e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;3.54003906e-02&nbsp;&nbsp;1.66992188e-01&nbsp;-7.27539062e-02&nbsp;-2.53906250e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.34765625e-01&nbsp;&nbsp;3.69140625e-01&nbsp;&nbsp;1.83593750e-01&nbsp;-1.64062500e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.26562500e-01&nbsp;-8.88671875e-02&nbsp;&nbsp;3.69140625e-01&nbsp; 5.54199219e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-3.63769531e-02&nbsp;-1.48437500e-01&nbsp;&nbsp;9.13085938e-02&nbsp; 2.47955322e-04<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.67578125e-01&nbsp;-1.63085938e-01&nbsp;&nbsp;1.19628906e-01&nbsp; 2.77343750e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.49414062e-01&nbsp;&nbsp;1.33789062e-01&nbsp;-8.25195312e-02&nbsp;-1.74804688e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.77734375e-01&nbsp;&nbsp;2.06054688e-01&nbsp;&nbsp;5.07812500e-02&nbsp;-2.08007812e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.74804688e-01&nbsp;&nbsp;9.66796875e-02&nbsp;&nbsp;6.98242188e-02&nbsp;-5.79833984e-04<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;9.22851562e-02&nbsp;&nbsp;7.95898438e-02&nbsp;&nbsp;1.41601562e-01&nbsp; 8.72802734e-03<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-8.05664062e-02&nbsp;&nbsp;4.80957031e-02&nbsp;&nbsp;2.49023438e-01&nbsp;-1.64062500e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-4.66308594e-02&nbsp;-2.81250000e-01&nbsp;-1.66015625e-01&nbsp;-2.22656250e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-2.32421875e-01&nbsp;&nbsp;1.32812500e-01&nbsp;&nbsp;4.15039062e-02&nbsp; 1.15234375e-01<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-7.66601562e-02&nbsp;-1.10839844e-01&nbsp;-1.97265625e-01&nbsp; 3.06396484e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.03515625e-01&nbsp;&nbsp;2.49023438e-02&nbsp;-2.52685547e-02&nbsp; 3.39355469e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;4.29687500e-02&nbsp;-1.44531250e-01&nbsp;&nbsp;2.12402344e-02&nbsp; 2.28271484e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.88476562e-01&nbsp;&nbsp;3.22265625e-01&nbsp;-1.13281250e-01&nbsp;-7.61718750e-02<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;&nbsp;2.94921875e-01&nbsp;-1.33789062e-01&nbsp;-1.80664062e-02&nbsp;-6.25610352e-03<br> </font>\n",
    ">  <font face='monospace' size=3>&nbsp;-1.62353516e-02&nbsp;&nbsp;5.98144531e-02&nbsp;&nbsp;1.21582031e-01&nbsp; 4.17480469e-02\\] </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(w2v_model[\"length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RL4Gqhyw56oX"
   },
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> You can also find the top-N most similar words. Try it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zq-Jwhxe5jDy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elementary', 0.7868632078170776),\n",
       " ('schools', 0.7411909103393555),\n",
       " ('shool', 0.6692329049110413),\n",
       " ('elementary_schools', 0.6597153544425964),\n",
       " ('kindergarten', 0.6529811024665833)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### print top 5 most similar words to \"school\"\n",
    "w2v_model.most_similar(positive=[\"school\"], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUUOFU4J4Anl"
   },
   "source": [
    "<font color=\"green\">Expected output: </font>\n",
    ">  <font face='monospace' size=3>\n",
    "[('elementary', 0.7868632078170776),<br>\n",
    "&nbsp;('schools', 0.7411909103393555),<br>\n",
    "&nbsp;('shool', 0.6692329049110413),<br>\n",
    "&nbsp;('elementary_schools', 0.6597153544425964),<br>\n",
    "&nbsp;('kindergarten', 0.6529811024665833)]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIk5hWfGeOlR"
   },
   "source": [
    "## Preprocessing\n",
    "Preprocess the two tsv files here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUKN7pEKeOlS"
   },
   "source": [
    "#### adjust the ratio of the two classes of training data\n",
    "In the training data, the ratio of good phrases to bad phrases is about one to thirty. That will make training classification unsatisfactory, so we need to adjust the ratio. Reducing bad phrases and adding good phrases are both common way.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please adjust the ratio of good phrases to bad phrases however you think is best and output the number of the two classes for demo.\n",
    "\n",
    "You need to explain why you chose this ratio and how you did it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "gpleKkC9eOlS",
    "outputId": "c8fe9973-707a-4b08-be22-c9532f76fe3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199598"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)\n",
    "#### print the number of training data of two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V40xvY1eOlT"
   },
   "source": [
    "#### number words\n",
    "Give each word a unique number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "N2g3NLJ9eOlT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 17:15:09.725730: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9149\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(pd.concat([train,test],ignore_index=True)['phrase'])\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sj0V1G_AeOlT"
   },
   "source": [
    "#### convert phrases into numbers\n",
    "Your model can't understand words, so we have to do this transform first. \n",
    "\n",
    "The number should be the same as the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FdwSF-1JeOlU"
   },
   "outputs": [],
   "source": [
    "train_encoded_phrase = tok.texts_to_sequences(train['phrase'])\n",
    "test_encoded_phrase = tok.texts_to_sequences(test['phrase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hhTKdm7eOlV"
   },
   "source": [
    "#### <font color=\"red\">**[ TODO ]**</font> padding\n",
    "Make all phrases the same length. The longest phrases in the two tsv files have five tokens. Hence, we should add zeroes to all the phrases that are shorter than five. \n",
    "- we suggest using `pad_sequences`, but you can do it however you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LF8rQwmneOlV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    1    3 1970  253]\n",
      " [7468   10 1971    1  122]\n",
      " [   0    0    2    1  119]\n",
      " [   0    2    1   43   64]\n",
      " [   0   23    1 1760 1507]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X_train = pad_sequences(train_encoded_phrase, maxlen=5)\n",
    "X_test = pad_sequences(test_encoded_phrase, maxlen=5)\n",
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbE9uyk0eOlW"
   },
   "source": [
    "#### <font color=\"red\">**[ TODO ]**</font> one hot encode the labels\n",
    "- we suggest using `to_categorical`, but again, you can use whatever you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SJkFyC8_eOlX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(train[\"class\"])\n",
    "y_test = to_categorical(test[\"class\"])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AJnIDUSeOlX"
   },
   "source": [
    "#### split training data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "r32haPqreOlY"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKthy0kTeOlY"
   },
   "source": [
    "#### <font color=\"red\">**[ TODO ]**</font> creating the embedding matrix\n",
    "The embedding matrix is used by the classification model. It should be a list of lists. Each sub-list is an embedding vector of a word and the order of all embedding vectors should be same as the word index numbering from the *tokenizer*. The tokenizer output is stored in a dictionary. You can check it using `tok.word_index.items()`.\n",
    "\n",
    "Make the embedding matrix. Our example model will need one, but you can skip it if the classification model you're using doesn't need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.sequences_to_texts([[2]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7zBQDNmmeOlZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([ 2.41699219e-02, -1.26953125e-01, -3.59375000e-01,  3.32031250e-01,\n",
      "        1.17187500e-01,  9.76562500e-02, -2.01416016e-02, -1.72851562e-01,\n",
      "       -4.54101562e-02,  2.63671875e-02, -1.35742188e-01, -6.22558594e-02,\n",
      "        1.23291016e-02,  4.17968750e-01, -1.06201172e-02,  3.80859375e-01,\n",
      "        1.64794922e-02,  5.93261719e-02, -5.93261719e-02,  1.08398438e-01,\n",
      "       -2.11914062e-01, -8.49609375e-02, -1.25976562e-01,  4.71191406e-02,\n",
      "        6.59179688e-02, -2.31445312e-01, -7.32421875e-02, -2.45117188e-01,\n",
      "       -2.13623047e-02,  1.69921875e-01,  1.72851562e-01, -1.66015625e-01,\n",
      "       -1.15722656e-01, -1.17187500e-01,  8.25195312e-02, -1.38671875e-01,\n",
      "        1.29882812e-01,  1.32812500e-01, -7.71484375e-02,  1.53320312e-01,\n",
      "        9.52148438e-02, -2.43164062e-01,  5.37109375e-02,  4.39453125e-02,\n",
      "       -1.65039062e-01, -4.14062500e-01,  1.32812500e-01, -5.34667969e-02,\n",
      "        9.71679688e-02,  6.10351562e-02, -8.00781250e-02,  6.78710938e-02,\n",
      "        3.65234375e-01,  4.88281250e-02, -2.91015625e-01,  9.96093750e-02,\n",
      "       -3.02124023e-03, -1.28906250e-01, -1.37695312e-01, -1.83593750e-01,\n",
      "        1.76757812e-01, -7.47070312e-02, -2.65625000e-01, -1.61132812e-01,\n",
      "       -6.44531250e-02,  4.22363281e-02,  6.29882812e-02, -2.50000000e-01,\n",
      "        1.03027344e-01,  1.19628906e-01,  5.37109375e-02,  1.19140625e-01,\n",
      "        7.17773438e-02, -3.85742188e-02,  2.94921875e-01, -4.02343750e-01,\n",
      "       -8.60595703e-03,  1.92382812e-01,  1.55029297e-02,  1.30615234e-02,\n",
      "       -1.63085938e-01,  6.98242188e-02, -2.24609375e-01,  3.28125000e-01,\n",
      "       -2.56347656e-02, -4.19921875e-02,  1.33789062e-01,  6.20117188e-02,\n",
      "        4.15039062e-02,  1.22558594e-01, -6.22558594e-02,  2.02148438e-01,\n",
      "       -8.10546875e-02, -9.66796875e-02,  5.95703125e-02, -3.20312500e-01,\n",
      "        3.65234375e-01,  8.25195312e-02,  8.78906250e-03, -2.19726562e-01,\n",
      "       -1.10839844e-01, -2.12890625e-01, -1.69921875e-01, -5.90820312e-02,\n",
      "        2.44140625e-02, -1.31835938e-01, -1.90429688e-01, -2.06054688e-01,\n",
      "        3.14941406e-02, -8.98437500e-02,  2.40234375e-01, -6.03027344e-02,\n",
      "       -1.35742188e-01,  2.89306641e-02,  1.62109375e-01,  1.64062500e-01,\n",
      "        1.59179688e-01,  2.18505859e-02, -1.90429688e-02, -1.68945312e-01,\n",
      "        2.75390625e-01, -2.08984375e-01, -1.63574219e-02,  1.13769531e-01,\n",
      "       -6.54296875e-02,  3.28063965e-04, -8.98437500e-02, -3.88183594e-02,\n",
      "       -2.06054688e-01, -6.17675781e-02,  1.51367188e-01, -7.47070312e-02,\n",
      "       -7.42187500e-02, -3.41796875e-02,  8.69140625e-02, -3.00292969e-02,\n",
      "       -3.00781250e-01,  4.14062500e-01, -1.66015625e-01,  2.25585938e-01,\n",
      "       -1.93359375e-01, -2.55859375e-01, -1.68945312e-01, -7.91015625e-02,\n",
      "        5.03540039e-03,  1.09863281e-02,  1.79443359e-02, -2.09960938e-01,\n",
      "       -6.25000000e-02, -1.61132812e-01,  2.12402344e-02,  1.66992188e-01,\n",
      "       -1.04492188e-01,  4.02832031e-03, -1.05957031e-01,  2.77343750e-01,\n",
      "        8.20312500e-02, -1.44195557e-03,  1.09375000e-01, -8.30078125e-02,\n",
      "        5.39550781e-02, -3.56445312e-02, -1.60156250e-01, -1.76757812e-01,\n",
      "       -7.56835938e-02,  7.99560547e-03,  5.29785156e-02, -1.90429688e-01,\n",
      "        8.54492188e-02, -2.25830078e-02, -3.10546875e-01, -2.35351562e-01,\n",
      "       -6.73828125e-02, -9.37500000e-02, -8.74023438e-02, -6.17675781e-02,\n",
      "        2.71484375e-01,  1.35742188e-01, -1.18164062e-01,  1.88476562e-01,\n",
      "       -1.16210938e-01, -2.31445312e-01,  7.66601562e-02,  3.69140625e-01,\n",
      "       -3.16406250e-01, -1.99218750e-01,  1.02050781e-01,  1.17187500e-01,\n",
      "        1.94091797e-02, -2.38037109e-02,  9.86328125e-02, -1.33789062e-01,\n",
      "       -1.07421875e-01, -2.46093750e-01, -3.33984375e-01, -1.03027344e-01,\n",
      "        2.09960938e-01, -1.00585938e-01, -4.33593750e-01, -1.06445312e-01,\n",
      "       -3.58886719e-02,  2.14843750e-01,  1.55273438e-01,  2.61230469e-02,\n",
      "       -1.39770508e-02, -1.68945312e-01, -3.47656250e-01, -1.58203125e-01,\n",
      "       -1.70898438e-01, -6.25000000e-02,  1.31835938e-01,  1.46484375e-01,\n",
      "        4.68750000e-02,  3.08593750e-01,  3.80859375e-02,  3.14453125e-01,\n",
      "        1.10473633e-02,  1.36718750e-01, -3.32031250e-02, -2.87109375e-01,\n",
      "       -2.30712891e-02,  5.54199219e-02,  2.02148438e-01,  2.29492188e-02,\n",
      "        1.72851562e-01,  3.20312500e-01, -1.29882812e-01, -1.60156250e-01,\n",
      "        1.86523438e-01, -1.13281250e-01,  3.47656250e-01, -1.12792969e-01,\n",
      "        4.08203125e-01,  1.85546875e-01,  1.34765625e-01,  6.34765625e-02,\n",
      "        6.17675781e-02, -1.10351562e-01,  1.39648438e-01,  3.51562500e-02,\n",
      "        9.91210938e-02, -1.43554688e-01,  2.12402344e-02, -3.00781250e-01,\n",
      "       -1.66992188e-01,  1.42578125e-01,  7.95898438e-02, -1.19628906e-01,\n",
      "       -1.97265625e-01, -3.43750000e-01, -1.04980469e-01, -3.66210938e-03,\n",
      "       -6.29882812e-02, -2.58789062e-02, -1.57226562e-01,  3.45703125e-01,\n",
      "       -9.39941406e-03,  1.20117188e-01, -1.29882812e-01,  2.02636719e-02,\n",
      "        1.68945312e-01, -2.38037109e-02, -3.11279297e-02,  2.20703125e-01,\n",
      "        2.65625000e-01,  1.12792969e-01,  2.14843750e-01, -6.59179688e-02,\n",
      "        2.16064453e-02, -1.37695312e-01, -6.83593750e-02, -1.44531250e-01,\n",
      "        1.66015625e-01,  8.54492188e-02,  2.55859375e-01,  1.23046875e-01,\n",
      "       -2.81250000e-01, -2.94921875e-01,  1.18408203e-02,  3.63769531e-02,\n",
      "        2.09960938e-01, -2.29492188e-02, -1.05468750e-01,  1.66015625e-01,\n",
      "       -1.78222656e-02, -1.68457031e-02, -5.81054688e-02,  2.73437500e-01,\n",
      "       -2.53906250e-02,  8.69140625e-02,  1.38671875e-01,  2.28515625e-01,\n",
      "       -2.23632812e-01, -2.42187500e-01, -2.27539062e-01, -6.25000000e-02,\n",
      "        2.38281250e-01, -2.03125000e-01,  2.38281250e-01, -1.53320312e-01],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0.20410156,  0.01318359,  0.07568359,  0.28515625, -0.10888672,\n",
      "        0.10107422, -0.02954102,  0.0480957 , -0.11132812, -0.00326538,\n",
      "       -0.09277344, -0.05761719, -0.12988281, -0.11132812, -0.24707031,\n",
      "        0.140625  ,  0.07470703,  0.02661133,  0.23632812, -0.06689453,\n",
      "        0.02856445,  0.09082031,  0.19140625, -0.08251953, -0.02978516,\n",
      "        0.11425781, -0.03442383,  0.00344849, -0.00102234, -0.05444336,\n",
      "       -0.05615234,  0.09033203, -0.03637695, -0.05761719, -0.20898438,\n",
      "       -0.02893066,  0.09765625,  0.05200195,  0.04125977,  0.19238281,\n",
      "        0.02331543, -0.24707031,  0.34765625,  0.00479126,  0.10791016,\n",
      "        0.11279297,  0.08007812, -0.06689453,  0.15136719, -0.1796875 ,\n",
      "        0.00775146,  0.18066406,  0.0246582 ,  0.08691406, -0.01257324,\n",
      "       -0.12792969, -0.00315857, -0.03088379,  0.12792969,  0.13476562,\n",
      "        0.06298828, -0.00479126, -0.13867188, -0.05810547,  0.02160645,\n",
      "       -0.06079102, -0.13378906,  0.07763672, -0.08496094, -0.08740234,\n",
      "        0.2734375 ,  0.07177734,  0.06030273, -0.22851562, -0.35742188,\n",
      "       -0.265625  ,  0.00294495,  0.24023438,  0.04150391,  0.3203125 ,\n",
      "       -0.12011719, -0.22460938,  0.08154297,  0.04296875, -0.09765625,\n",
      "       -0.13867188,  0.10107422,  0.27539062, -0.00379944,  0.11425781,\n",
      "        0.006073  ,  0.13085938, -0.16894531, -0.07373047, -0.11035156,\n",
      "       -0.06494141,  0.30078125,  0.09179688,  0.06298828, -0.04711914,\n",
      "       -0.13378906, -0.00090408, -0.01806641,  0.0703125 , -0.3828125 ,\n",
      "        0.04492188, -0.15917969,  0.06030273, -0.01519775, -0.23046875,\n",
      "       -0.25390625, -0.12451172, -0.09814453, -0.02478027,  0.07470703,\n",
      "       -0.09472656, -0.09912109, -0.11767578,  0.15136719, -0.09960938,\n",
      "       -0.00430298, -0.20214844, -0.06591797,  0.14453125,  0.06933594,\n",
      "        0.06542969, -0.10693359,  0.07519531, -0.02001953,  0.07128906,\n",
      "       -0.3359375 , -0.15039062, -0.21875   ,  0.0859375 ,  0.0859375 ,\n",
      "       -0.21484375, -0.05541992, -0.03271484,  0.06079102,  0.33789062,\n",
      "        0.09130859, -0.10107422, -0.07568359, -0.02990723,  0.1953125 ,\n",
      "       -0.140625  ,  0.03833008, -0.3046875 ,  0.171875  , -0.01428223,\n",
      "        0.19335938,  0.09667969, -0.26757812, -0.11523438, -0.04345703,\n",
      "        0.06542969,  0.02502441, -0.0111084 , -0.17675781,  0.19238281,\n",
      "       -0.00270081,  0.20019531, -0.14941406,  0.24511719,  0.01397705,\n",
      "       -0.28515625,  0.171875  , -0.07324219,  0.03930664, -0.02697754,\n",
      "       -0.29101562,  0.05151367, -0.05249023, -0.14648438, -0.13183594,\n",
      "        0.02722168,  0.12207031, -0.05371094, -0.04760742,  0.01940918,\n",
      "       -0.05859375,  0.11572266,  0.05053711,  0.00042725, -0.06005859,\n",
      "       -0.0133667 , -0.13867188, -0.01574707,  0.11962891,  0.04833984,\n",
      "       -0.06542969,  0.20898438, -0.14355469, -0.02514648,  0.109375  ,\n",
      "        0.11083984, -0.07470703, -0.02453613, -0.07568359, -0.18457031,\n",
      "        0.05444336,  0.12890625, -0.10205078,  0.10498047, -0.02734375,\n",
      "        0.17578125, -0.25976562, -0.20214844, -0.04467773,  0.03588867,\n",
      "        0.02661133,  0.17480469, -0.16894531,  0.03540039, -0.30273438,\n",
      "       -0.04858398,  0.19238281,  0.06542969, -0.22070312,  0.06225586,\n",
      "       -0.07128906, -0.05761719, -0.15039062, -0.18261719,  0.11621094,\n",
      "       -0.0625    ,  0.18652344,  0.00308228, -0.23730469, -0.07861328,\n",
      "       -0.05273438, -0.03344727, -0.02380371,  0.11474609,  0.03015137,\n",
      "        0.03369141, -0.06298828,  0.05810547,  0.13964844, -0.04150391,\n",
      "        0.01025391,  0.08789062, -0.00175476, -0.17578125, -0.04858398,\n",
      "       -0.10058594, -0.11083984,  0.27148438, -0.04272461, -0.1875    ,\n",
      "        0.09912109, -0.10302734, -0.04882812,  0.05200195, -0.01470947,\n",
      "       -0.00756836,  0.06079102, -0.20019531, -0.05371094, -0.01391602,\n",
      "       -0.03686523, -0.20507812, -0.18554688, -0.11376953,  0.02453613,\n",
      "        0.13671875,  0.03112793, -0.00534058, -0.17480469, -0.02258301,\n",
      "       -0.10791016,  0.34375   ,  0.16113281,  0.30273438, -0.09082031,\n",
      "       -0.02685547,  0.00588989, -0.19335938, -0.14257812,  0.05004883,\n",
      "        0.01733398, -0.19140625, -0.00897217,  0.08740234, -0.06787109,\n",
      "        0.13476562, -0.06103516, -0.203125  ,  0.00958252,  0.16601562,\n",
      "       -0.01635742,  0.1328125 , -0.23828125, -0.10595703,  0.08691406,\n",
      "       -0.10058594,  0.04956055, -0.21191406, -0.1328125 ,  0.10839844],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_matrix = []\n",
    "for idx in range(0, vocab_size):\n",
    "  word = tok.sequences_to_texts([[idx]])[0]\n",
    "  if word in w2v_model.index_to_key:\n",
    "    embedding_matrix.append(w2v_model[word])\n",
    "  else:\n",
    "    embedding_matrix.append(np.zeros(w2v_model.vector_size))\n",
    "print(embedding_matrix[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c9JvMZaeOlZ"
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpgeqgmWeOlZ"
   },
   "source": [
    "#### build model\n",
    "<font color=\"red\">**[ TODO ]**</font> Please build your classification model by ***keras*** here. Don't worry if you don't know how, just use the one given below. Feel free to make any changes or even build your own.\n",
    "\n",
    "You **must** use the pre-trained word2vec model to represent the words of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XRlAGm9teOla"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 17:24:09.436259: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/miniconda3/envs/hw4env/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Flatten , Embedding, LSTM, LSTM, ReLU, Dropout\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,output_dim=300,input_length=5,embeddings_initializer=Constant(embedding_matrix)))\n",
    "model.add(LSTM(64,return_sequences=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2,activation='sigmoid')) \n",
    "model.compile(optimizer=RMSprop(lr=1e-3),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7EVyMofjeOla"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 5, 300)            2744700   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                93440     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,838,270\n",
      "Trainable params: 2,838,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSG0J5P7eOla"
   },
   "source": [
    "#### train\n",
    "Train classification model here.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Adjust the hyperparameter to optimize the validation accuracy and validation loss.\n",
    "\n",
    "* The higher the accuracy, the better; the lower the validation, the better.\n",
    "* **number of epoch** and **batch size** are the most important\n",
    "  * Start with a smaller number of epochs first--it is directly correlated to the training time, and you don't want to spend too much time waiting!\n",
    "  * Usually the larger the batch size the better, but the batch size you are able to use depends on you computing power, so start small and increase gradually. It is recommended to use powers of 2 (2, 4, 8, 16, ...) for batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MSs4f9ELeOlb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "79839/79839 [==============================] - 1885s 24ms/step - loss: 0.0826 - accuracy: 0.9766 - val_loss: 0.0781 - val_accuracy: 0.9788\n",
      "Epoch 2/3\n",
      "79839/79839 [==============================] - 2332s 29ms/step - loss: 0.0657 - accuracy: 0.9825 - val_loss: 0.0662 - val_accuracy: 0.9838\n",
      "Epoch 3/3\n",
      "79839/79839 [==============================] - 693s 9ms/step - loss: 0.0565 - accuracy: 0.9865 - val_loss: 0.0534 - val_accuracy: 0.9868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f80997b2e20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,validation_data=(X_val,y_val),batch_size=2,epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJIEsewSeOlc"
   },
   "source": [
    "#### test\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Test your model by test.tsv and output the accuracy. Beat the accuracy baseline: **0.98** for extra points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "nDzmeBV4eOlc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 3ms/step - loss: 0.3176 - accuracy: 0.9235\n",
      "0.9235000014305115\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(X_test,y_test)\n",
    "print(accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxVkZUuFeOlc"
   },
   "source": [
    "## Show wrong prediction results\n",
    "Observing wrong prediction result may help you improve your prediction.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> show the wrong prediction results like this: \n",
    "\n",
    "<img src=\"https://imgur.com/BOTMyZH.jpg\" width=30%><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVqH4lvleOld"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KprbNm7KeOle"
   },
   "source": [
    "## TA's Notes\n",
    "\n",
    "If you complete the Assignment, please use [this link](https://docs.google.com/spreadsheets/d/1OKbXhcv6E3FEQDPnbHEHEeHvpxv01jxugMP7WwnKqKw/edit#gid=258852025) to reserve demo time.  \n",
    "The score is only given after TAs review your implementation, so <u>**make sure you make a appointment with a TA before you miss the deadline**</u> .  <br>After demo, please upload your assignment to elearn. You just need to hand in this ipynb file and rename it as XXXXXXXXX(Your student ID).ipynb.\n",
    "<br>Note that **late submission will not be allowed**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQl6lTDzeOlf"
   },
   "source": [
    "## Learning Resource\n",
    "[Deep Learning with Python](https://tanthiamhuat.files.wordpress.com/2018/03/deeplearningwithpython.pdf)\n",
    "\n",
    "[Classification on IMDB](https://keras.io/examples/nlp/bidirectional_lstm_imdb/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
