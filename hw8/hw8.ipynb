{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7L3cYlZl15_K"
   },
   "source": [
    "###### Week 9: Sentence Level Classification with BERT\n",
    "\n",
    "Your goal this week is to train a classifier that can predict the CEFR level of any given sentence. In this notebook we will guide you through the process of using ðŸ¤—[Hugging Face](https://huggingface.co/) and its transformers library as the training framework, with [Pytorch](https://pytorch.org/) as the deep learning backend, but feel free to use [TensorFlow](https://www.tensorflow.org) if that's what you are more familiar with.\n",
    "\n",
    "For this assignment we will provide a dataset containing sentences with the corresponding CEFR level, and you have to use BERT and train a sentence classifier with this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8TbxtroCxM8"
   },
   "source": [
    "## Prepare your environment\n",
    "\n",
    "As always, we highly recommend that you install all packages with a virtual environment manager, like [venv](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) or [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html), to prevent version conflicts of different packages.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSHP0CPoXj7Z"
   },
   "source": [
    "### Install CUDA\n",
    "Deep learning is a computionally extensive process. It takes lots of time if relying only on the CPU, especially when it's trained on a large dataset. That's why using GPU instead is generally recommended.  \n",
    "To use GPU for computation, you have to install [CUDA toolkit](https://developer.nvidia.com/cuda-toolkit) as well as the [cuDNN library](https://developer.nvidia.com/cudnn) provided by NVIDIA.  \n",
    "\n",
    "If you already had CUDA installed on your machine, then great! You're done here.  \n",
    "If you don't, you can refer to [Appendix](#Appendix-1-Install-CUDA) to see how to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f78jLXeZfPyH"
   },
   "source": [
    "\n",
    "### Install python packages\n",
    "The following python packages will be used in this tutorial:\n",
    "\n",
    "1. `numpy`: for matrix operation\n",
    "2. `scikit-learn`: for label encoding\n",
    "3. `datasets`: for data preparation\n",
    "4. `transformers`: for model loading and finetuing\n",
    "5. `pytorch`: the backend DL framework\n",
    "  - Note that the pt version must support the CUDA version you've installed if you want to use GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cS9CxjyfQ-F"
   },
   "source": [
    "### Select GPU(s) for your backend\n",
    "\n",
    "Skip this section if you have no intension of using GPU with tensorflow/pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sEJ8Y8SCfWp_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# select your GPU. Note that this should be set before you load tensorflow or pytorch.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# To use multiple GPUs, combine all GPU ID with commas\n",
    "# e.g. >>> os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1p_qQKbfcCH",
    "outputId": "d25bdd87-959c-435d-8c57-22f5c350098f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Check if any GPU is used\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P0foxjBDQSu"
   },
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "Before starting the training, we need to load and process our dataset - but wait, let's decide which model we want to use first.  \n",
    "\n",
    "In the highly unlikely chance you've never heard of it, [BERT](https://arxiv.org/abs/1810.04805) (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) is a language model proposed by Google AI in 2018, and it's currently one of the most popular models used in NLP.  \n",
    "You can learn more about it here:\n",
    "- [BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/) by Samia, 2019.\n",
    "\n",
    "\n",
    "However, we will not directly use BERT in this tutorial, because it's large and takes too long to train. Instead, we'll be using [DistilBert](https://medium.com/huggingface/distilbert-8cf3380435b5), a version of BERT that while light-weight, reserves 95% of its original accuracy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Lqb2cHCmDIEp"
   },
   "outputs": [],
   "source": [
    "# the model you want to use. Available models can be found here: https://huggingface.co/models\n",
    "MODEL_NAME = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxSUKsTkDSxJ"
   },
   "source": [
    "### Load data\n",
    "\n",
    "Similar to the `transformers` library, `datasets` is also a package by huggingface. It contains many public datasets online and can help us with the data processing.  \n",
    "We can use `load_dataset` function to read the input `.csv` file provided for this assignment.\n",
    "\n",
    "Reference:\n",
    " - [Official datasets document](https://huggingface.co/docs/datasets)\n",
    " - [datasets.load_dataset](https://huggingface.co/docs/datasets/loading.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hjY9HMNIt5jf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-942e063d8014240f\n",
      "Found cached dataset csv (/home/bill/.cache/huggingface/datasets/csv/default-942e063d8014240f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5ec664e6644d16a4c3b1c64ef1f2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'level'],\n",
       "        num_rows: 23020\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"data\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npepcv7GfMHI",
    "outputId": "02ed2940-370f-444c-b748-abcd623891bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'level'],\n",
      "    num_rows: 23020\n",
      "})\n",
      "{'text': \"Unfortunately he was too fast and I couldn't keep up with him.\", 'level': 'B2'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"])\n",
    "print(dataset[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No longer a remote, backward, unimportant country, it became a force to be reckoned with in Europe.', \"Unfortunately he was too fast and I couldn't keep up with him.\", 'Most mushrooms are totally harmless, but some are poisonous.', 'This provided solid evidence that he committed the crime.', \"You can't just accept everything you read in the newspapers at face value.\"]\n",
      "['C2', 'B2', 'B2', 'C2', 'C1']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][\"text\"][:5])\n",
    "print(dataset[\"train\"][\"level\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3olKw19uuJQ"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "As always, texts should be tokenized, embedded, and padded before being put into the model.  \n",
    "But not to worry, there are libraries from huggingface to help with this, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3ANJAV7wn2R"
   },
   "source": [
    "#### Sentence processing\n",
    "\n",
    "Different pre-trained language models may have their own preprocessing models, and that's why we should use the tokenizers trained along with that model. In our case, we are using distilBERT, so we should use the distilBERT tokenizer.  \n",
    "\n",
    "With huggingface, loading different tokenizers is extremely easy: just import the AutoTokenizer from `transformers` and tell it what model you plan to use, and it will handle everything for you.\n",
    "\n",
    "Reference:\n",
    " - [transformers.AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DALrmSh6wpmy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 6854, 2002, 2001, 2205, 3435, 1998, 1045, 2481, 1005, 1056, 2562, 2039, 2007, 2032, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer(\"Unfortunately he was too fast and I couldn't keep up with him.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd43REmNxCOV"
   },
   "source": [
    "#### Label processing\n",
    "\n",
    "Our labels also need to be processed, so let's do that next.\n",
    "\n",
    "For this tutorial, we'll use the OneHotEncoder provided by scikit-learn.\n",
    "\n",
    "For now, just declare a new encoder and use `fit` to learn the data. Hint: you should still end up with 6 labels.\n",
    "\n",
    "Documents:\n",
    " - [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No longer a remote, backward, unimportant coun...</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unfortunately he was too fast and I couldn't k...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Most mushrooms are totally harmless, but some ...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This provided solid evidence that he committed...</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You can't just accept everything you read in t...</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text level\n",
       "0  No longer a remote, backward, unimportant coun...    C2\n",
       "1  Unfortunately he was too fast and I couldn't k...    B2\n",
       "2  Most mushrooms are totally harmless, but some ...    B2\n",
       "3  This provided solid evidence that he committed...    C2\n",
       "4  You can't just accept everything you read in t...    C1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  level\n",
       "0    C2\n",
       "1    B2\n",
       "2    B2\n",
       "3    C2\n",
       "4    C1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"level\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "q3ml0_WxxFh5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['A1', 'A2', 'B1', 'B2', 'C1', 'C2'], dtype=object)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder().fit(df[[\"level\"]])\n",
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRY2GDJa1MxF",
    "outputId": "df210570-a2f8-478d-8902-e6f6c4f33af5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if you still have 6 labels\n",
    "LABELS_NUM = len(encoder.categories_[0])\n",
    "LABELS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_levels = encoder.transform(df[[\"level\"]]).toarray()\n",
    "encoded_levels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.transform(pd.DataFrame({\"level\": [\"B2\"]})).toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrfO8c4R1eWO"
   },
   "source": [
    "#### Process the data\n",
    "\n",
    "To make things easier, we can write a function to process our dataset in batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cvwnYLah1dbN"
   },
   "outputs": [],
   "source": [
    "def preprocess(dataslice):\n",
    "  \"\"\" Input: a batch of your dataset\n",
    "      Example: { 'text': [['sentence1'], ['sentence2'], ...],\n",
    "                 'level': ['label1', 'label2', ...] }\n",
    "  \"\"\"\n",
    "  tokenized_inputs = tokenizer(dataslice[\"text\"])\n",
    "  labels = []\n",
    "  for level in dataslice[\"level\"]:\n",
    "    encoded_level = encoder.transform(pd.DataFrame({\"level\": [level]})).toarray()[0]\n",
    "    labels.append(encoded_level)\n",
    "  tokenized_inputs[\"label\"] = labels\n",
    "  return tokenized_inputs\n",
    "  \"\"\" Output: a batch of processed dataset\n",
    "      Example: { 'input_ids': ...,\n",
    "                 'attention_mask': ...,\n",
    "                 'label': ... }\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Nr0-Y1bR2efQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bill/.cache/huggingface/datasets/csv/default-942e063d8014240f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f6760c99273257a6.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'level', 'input_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 23020\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map the function to the whole dataset\n",
    "processed_data = dataset.map(preprocess, batched = True)\n",
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_G5ftJbjfa4i",
    "outputId": "c88d0fc1-85ca-4cdb-e191-a426e29fcfc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Unfortunately he was too fast and I couldn't keep up with him.\",\n",
       " 'level': 'B2',\n",
       " 'input_ids': [101,\n",
       "  6854,\n",
       "  2002,\n",
       "  2001,\n",
       "  2205,\n",
       "  3435,\n",
       "  1998,\n",
       "  1045,\n",
       "  2481,\n",
       "  1005,\n",
       "  1056,\n",
       "  2562,\n",
       "  2039,\n",
       "  2007,\n",
       "  2032,\n",
       "  1012,\n",
       "  102],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'label': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['train'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9z7ZMtP22b9"
   },
   "source": [
    "### DataCollator\n",
    "\n",
    "You might have noticed that we skipped padding the sentences. That's because we are going to do it during training.  \n",
    "\n",
    "To do training-time processing, we can use the DataCollator Class provided by `transformers`. And guess what - transformers has a class that will handle padding for us, too!\n",
    "\n",
    " - [transformers.DataCollatorWithPadding](https://huggingface.co/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "x5orGYjN39dz"
   },
   "outputs": [],
   "source": [
    "# declare a collator to do padding during traning\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "di75QVgv4V81"
   },
   "source": [
    "## Training\n",
    "\n",
    "Finally, we can move on to training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53dO3pg85u0n"
   },
   "source": [
    "### Preparation\n",
    "\n",
    "We can load the pretrained model from `transformers`.  \n",
    "Generally, you need to build your own model on top of BERT if you want to use BERT for some downstream tasks, but again, sequence classification is a popular topic. With the support from `transformers` library, it can be done in two lines of codes: \n",
    "\n",
    "1. Load `AutoModelForSequenceClassification` Class.\n",
    "2. Load the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UyDyv7wp5qdD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels = LABELS_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtmP4TEh6XiB"
   },
   "source": [
    "#### Split train/val data\n",
    "\n",
    "The `Dataset` class we prepared before has a `train_test_split` method. You can use it to split your (processed) dataset.\n",
    "\n",
    "Document:\n",
    " - [datasets.Dataset - Sort, shuffle, select, split, and shard](https://huggingface.co/docs/datasets/process.html#sort-shuffle-select-split-and-shard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NnbD1KW16YWn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/bill/.cache/huggingface/datasets/csv/default-942e063d8014240f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f50113bbaf795ae7.arrow and /home/bill/.cache/huggingface/datasets/csv/default-942e063d8014240f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-325c892c50db8cac.arrow\n"
     ]
    }
   ],
   "source": [
    "# choose a validation size and split your data\n",
    "train_val_dataset = processed_data[\"train\"].train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6u93fCpofgbe",
    "outputId": "65b7044c-6a01-4c07-f456-75d0e83d9d70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'level', 'input_ids', 'attention_mask', 'label'],\n",
      "        num_rows: 20718\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'level', 'input_ids', 'attention_mask', 'label'],\n",
      "        num_rows: 2302\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzmSbOaD7O0F"
   },
   "source": [
    "#### Setup training parameters\n",
    "\n",
    "We are using the TrainerAPI to do the training. Trainer is yet another utility provided by huggingface, which helps you train the model with ease.  \n",
    "\n",
    "Document:\n",
    "- [transformers.TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments)\n",
    "- [transformers.Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ABqlinlO76Ax"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5lzXTG1y7q7n"
   },
   "outputs": [],
   "source": [
    "# set and tune your training properties\n",
    "OUTPUT_DIR = \"./trained_model/\"\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 20\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=OUTPUT_DIR,\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  per_device_train_batch_size=BATCH_SIZE,\n",
    "  per_device_eval_batch_size=BATCH_SIZE,\n",
    "  num_train_epochs=EPOCH\n",
    ")\n",
    "\n",
    "# now give all the information to a trainer\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=train_val_dataset[\"train\"],\n",
    "  eval_dataset=train_val_dataset[\"test\"],\n",
    "  tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gE0DpS3s7rhg"
   },
   "source": [
    "### Training\n",
    "\n",
    "This is the easy part. Simply ask the trainer to train the model for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "wsrQOyJCiFas"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, level. If text, level are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/bill/miniconda3/envs/hw8env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20718\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3240\n",
      "  Number of trainable parameters = 66958086\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3240' max='3240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3240/3240 14:03, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.386400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.299600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.179000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./trained_model/checkpoint-500\n",
      "Configuration saved in ./trained_model/checkpoint-500/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model/checkpoint-1000\n",
      "Configuration saved in ./trained_model/checkpoint-1000/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model/checkpoint-1500\n",
      "Configuration saved in ./trained_model/checkpoint-1500/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model/checkpoint-2000\n",
      "Configuration saved in ./trained_model/checkpoint-2000/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model/checkpoint-2500\n",
      "Configuration saved in ./trained_model/checkpoint-2500/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model/checkpoint-3000\n",
      "Configuration saved in ./trained_model/checkpoint-3000/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model = True\n",
    "if train_model:\n",
    "  trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JBUA0pe-S9b"
   },
   "source": [
    "### Save for future use\n",
    "\n",
    "Hint: try using `save_pretrained`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e30uXCf-cXc",
    "outputId": "f003dfa4-060a-4419-9ba6-f559029e5402"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./trained_model/config.json\n",
      "Model weights saved in ./trained_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "save_model = False\n",
    "if save_model:\n",
    "  model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QSZjlcG9fOk"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Now we know exactly how to train a model, but how do we use it for predicting results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7akP5Hh9ugG"
   },
   "source": [
    "### Load finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Lfb_zJGm9vJP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./trained_model/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./trained_model/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./trained_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./trained_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load the model that you saved\n",
    "\n",
    "mymodel = AutoModelForSequenceClassification.from_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6Vs3iBE_nck"
   },
   "source": [
    "### Get the prediction\n",
    "\n",
    "Here are a few example sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "FtLt6IBi_pLF"
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    # A2\n",
    "    \"Remember to write me a letter.\",\n",
    "    # B2\n",
    "    \"Strawberries and cream - a perfect combination.\",\n",
    "    \"This so-called \\\"Perfect Evening\\\" was so disappointing, as well as discouraging us from coming to your Circle Theatre again.\",\n",
    "    # C1\n",
    "    \"Some may altogether give up their studies, which I think is a disastrous move.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xhk7ZRX_2U2"
   },
   "source": [
    "All we need to do is to transform them to embeddings, and then we can get predictions by calling your finetuned model.  \n",
    "\n",
    "Since we don't have a DataCollator to pad the sentence and do the matrix transformation this time, we have to pad and transform the matrice on our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWONG_lCAkyN",
    "outputId": "0c408ae7-997d-4c0c-8f49-c6196397602f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7418,  0.7113, -3.2214, -4.4236, -4.0650, -4.0229],\n",
       "        [-5.7855, -5.3816, -3.4287,  1.6458, -2.5465, -2.3510],\n",
       "        [-5.5406, -4.9264, -1.6752,  1.4860, -3.0862, -4.9147],\n",
       "        [-5.9225, -5.8372, -4.5457,  1.4580, -1.6507, -3.1034]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the sentences into embeddings\n",
    "model_input = tokenizer(examples, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "# Get the output\n",
    "logits = mymodel(**model_input).logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBMUBD-1BFW1"
   },
   "source": [
    "Logits aren't very readable for us. Let's use softmax \n",
    "activation to transform them into more probability-like numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjiKxLaBBGah",
    "outputId": "006d24be-ed4f-48cb-ab2c-fa7b70b44dd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8318e-01, 7.8337e-01, 1.5347e-02, 4.6124e-03, 6.6013e-03, 6.8857e-03],\n",
       "        [5.6897e-04, 8.5212e-04, 6.0065e-03, 9.6041e-01, 1.4514e-02, 1.7646e-02],\n",
       "        [8.4011e-04, 1.5527e-03, 4.0092e-02, 9.4616e-01, 9.7791e-03, 1.5710e-03],\n",
       "        [5.8867e-04, 6.4106e-04, 2.3323e-03, 9.4439e-01, 4.2179e-02, 9.8666e-03]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "predicted_probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "predicted_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAqgoJTFBchb"
   },
   "source": [
    "#### Transform logits back to labels\n",
    "\n",
    "Now you've got the output. Write a function to map it back into labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvcyotBfBifR",
    "outputId": "50cae16f-06ed-4f04-c684-496044f37233"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A1', 'A2', 'B1', 'B2', 'C1', 'C2'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = encoder.categories_[0]\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A2', 'B2', 'B2', 'B2']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_labels(predicted_probabilities):\n",
    "  return [categories[label_idx] for label_idx in np.argmax(predicted_probabilities.detach().numpy(), axis=1)]\n",
    "\n",
    "get_labels(predicted_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfCuP95IBvP-"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's see how you did!  \n",
    "Load the testing data and calculate your accuracy.\n",
    "\n",
    "We want you to calculate the three kinds of accuracy mentioned in the lecture, which will also be explained in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She is of French nationality.',\n",
       " 'Ice skating, as an Olympic competition, was introduced in 1908.',\n",
       " 'So, these two experiences from my childhood taught me a lot of real truth about life and since that time they have been serving me as a measure of my affection or attachment, sorrow or disappointment.',\n",
       " 'Thankfully, no one was harmed in the accident.',\n",
       " 'He had a narrow escape when a falling tree crushed his car.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test data\n",
    "examples = train_val_dataset[\"test\"][\"text\"]\n",
    "examples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "model_input = tokenizer(examples, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "logits = mymodel(**model_input).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "C8hUc4sLCW40"
   },
   "outputs": [],
   "source": [
    "# get predictions\n",
    "predicted_probabilities = nn.functional.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform predictions back into labels\n",
    "predicted_labels = get_labels(predicted_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoC2EAHxCXdj",
    "outputId": "cc711f3c-c789-4d0f-d92f-adf04dc54432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1: She is of French nationality.\n",
      "B2: Ice skating, as an Olympic competition, was introduced in 1908.\n",
      "C2: So, these two experiences from my childhood taught me a lot of real truth about life and since that time they have been serving me as a measure of my affection or attachment, sorrow or disappointment.\n",
      "C1: Thankfully, no one was harmed in the accident.\n",
      "C2: He had a narrow escape when a falling tree crushed his car.\n",
      "B2: The theme of loss runs through most of his novels.\n",
      "C2: First of all, some people treasure certain possessions since they are valuable in its literal meaning, like diamonds or gold.\n",
      "A2: I must look like the typical tourist with my shorts and my camera.\n",
      "C1: It might be that the object in question reminds the owner of a beloved person, a deceased relative, a lost love, or a trip with his or her spouse.\n",
      "C2: Undoubtedly, the human rights' defenders would protest.\n"
     ]
    }
   ],
   "source": [
    "#  try printing out some predictions to check if the outputs are reasonable and if you need to adjust your model at the end of every step.\n",
    "for idx, (sent, level) in enumerate(zip(examples, predicted_labels)):\n",
    "  if idx >= 10:\n",
    "    break\n",
    "  print(f'{level}: {sent}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBnEzAFhC7ZN"
   },
   "source": [
    "### Six Level Accuracy\n",
    "\n",
    "Exact accuracy is probably what you're most familiar with:\n",
    "\n",
    "$\n",
    "accuracy = \\frac{\\#exactly\\:the\\:same\\:levels}{\\#total}\n",
    "$\n",
    "\n",
    "Example:\n",
    "```\n",
    "Prediction:   A1 A2 B1 B2 C1 C2\n",
    "Ground truth: A2 B1 B1 B2 B2 C2\n",
    "                    ^  ^     ^\n",
    "```\n",
    "\n",
    "The six level accuracy is $\\frac{3}{6} = 0.5$\n",
    "\n",
    "As the requirement, <u>your exact accuracy should be higher than $0.5$</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR2oVECyC8vD",
    "outputId": "72cebbe0-5ada-446b-b5f7-6a9849292792"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5195482189400521"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "corrects_num = 0\n",
    "for idx in range(len(predicted_labels)):\n",
    "  if predicted_labels[idx] == train_val_dataset[\"test\"][\"level\"][idx]:\n",
    "    corrects_num = corrects_num + 1\n",
    "corrects_num / len(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "We865ayVC_N7"
   },
   "source": [
    "### Three Level Accuracy\n",
    "\n",
    "Three Level Accuracy is used when you only want a more general sense of right or wrong.\n",
    "\n",
    "$\n",
    "accuracy = \\frac{\\#the\\:same\\:ABC\\:levels}{\\#total}\n",
    "$\n",
    "\n",
    "Example:\n",
    "```\n",
    "Prediction:   A1 A2 B1 B2 C1 C2\n",
    "Ground truth: A2 B1 B1 B2 B2 C2\n",
    "              ^     ^  ^     ^\n",
    "```\n",
    "\n",
    "The three level accuracy is $\\frac{4}{6} = 0.667$\n",
    "\n",
    "As the requirement, <u>your exact accuracy should be higher than $0.6$</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCAKM9MRDCBk",
    "outputId": "f599130e-de22-4b28-a521-f378afd52711"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7145960034752389"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "corrects_num = 0\n",
    "for idx in range(len(predicted_labels)):\n",
    "  if predicted_labels[idx][0] == train_val_dataset[\"test\"][\"level\"][idx][0]:\n",
    "    corrects_num = corrects_num + 1\n",
    "corrects_num / len(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YSo46NX7nvb"
   },
   "source": [
    "### Fuzzy accuracy\n",
    "\n",
    "However, the level of a sentence is relatively subjective. Generally speaking, $\\pm1$ errors are allowed in the real evaluation in linguistic area.  \n",
    "\n",
    "For example, if the actual label is 'B1', we'll also consider the prediction 'right' if the model predicts 'B2' or 'A2'.\n",
    "\n",
    "Hence, the fuzzy accuracy is\n",
    "\n",
    "$\n",
    "accuracy = \\frac{\\#good\\:enough\\:answers}{\\#total}\n",
    "$\n",
    "\n",
    "Example:\n",
    "```\n",
    "Prediction:   0 1 2 3 4 5\n",
    "Ground truth: 0 1 1 3 3 3\n",
    "              ^ ^ ^ ^ ^\n",
    "```\n",
    "\n",
    "The fuzzy accuracy is $\\frac{5}{6} = 0.833$\n",
    "\n",
    "As the requirement, <u>your accuracy should be higher than $0.8$</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27fP0BTc73Al",
    "outputId": "e792e699-4e04-4a82-ebff-d133aead0b60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A1': ['A1', 'A2'],\n",
       " 'A2': ['A2', 'A1', 'B1'],\n",
       " 'B1': ['B1', 'A2', 'B2'],\n",
       " 'B2': ['B2', 'B1', 'C1'],\n",
       " 'C1': ['C1', 'B2', 'C2'],\n",
       " 'C2': ['C2', 'C1']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz = {}\n",
    "for idx in range(len(categories)):\n",
    "  candidates = [categories[idx]]\n",
    "  if idx > 0:\n",
    "    candidates.append(categories[idx - 1])\n",
    "  if idx < len(categories) - 1:\n",
    "    candidates.append(categories[idx + 1])\n",
    "  fuzz[categories[idx]] = candidates\n",
    "fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8566463944396178"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrects_num = 0\n",
    "for idx in range(len(predicted_labels)):\n",
    "  if predicted_labels[idx] in fuzz[train_val_dataset[\"test\"][\"level\"][idx]]:\n",
    "    corrects_num = corrects_num + 1\n",
    "corrects_num / len(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPrSLQEDDfeE"
   },
   "source": [
    "## TA's Note\n",
    "\n",
    "Congratulations, you made it to the end of the tutorial! Make sure you make an appointment to show your work and turn in your finished assignment before next week's lesson. We will ask you to run your code, so double check that everything is working and that your model is saved. Don't worry if you didn't pass the evaluation requirements, you'll still get partial points for trying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL7CAtRQbR7s"
   },
   "source": [
    "## Appendix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqqUYYPEanAF"
   },
   "source": [
    "\n",
    "<a name=\"Appendix-1-Install-CUDA\"></a>\n",
    "\n",
    "### Appendix 1 - Install CUDA\n",
    "\n",
    "1. Check your GPU vs. CUDA compatibility:\n",
    "   - [NVIDIA -> Your GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) -> GeForce and TITAN Products\n",
    "2. Check library vs. CUDA compatibility: \n",
    "   - Pytorch: [Previous PyTorch Versions](https://pytorch.org/get-started/previous-versions/)\n",
    "   - Tensorflow: [Linux/MacOX](https://www.tensorflow.org/install/source#tested_build_configurations) or [Windows](https://www.tensorflow.org/install/source_windows#tested_build_configurations)\n",
    "3. Note the highest CUDA version that fits your system.\n",
    "\n",
    "#### >> for conda/mamba users\n",
    "\n",
    "You can directly install CUDA library with the selected CUDA version.\n",
    "1. Get [the driver for NVIDIA GPU](https://www.nvidia.com/download/index.aspx)\n",
    "2. `conda/mamba install -c conda-forge cudatoolkit=${VERSION}`\n",
    "\n",
    "#### >> for non-conda users\n",
    "\n",
    "1. Get [the driver for NVIDIA GPU](https://www.nvidia.com/download/index.aspx)\n",
    "2. Download and install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive)\n",
    "3. Download and install [cuDNN Library](https://developer.nvidia.com/rdp/cudnn-archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hFAM1Cya4_c"
   },
   "source": [
    "### Appendix 2 - Further Readings\n",
    "\n",
    "1. [Huggingface Official Tutorials](https://github.com/huggingface/notebooks/tree/master/examples)\n",
    "2. How to use Bert with other downstream tasks: [How to use BERT from the Hugging Face transformer library](https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209): \n",
    "3. Training with pytorch backend: [transformers-tutorials](https://github.com/abhimishra91/transformers-tutorials)\n",
    "4. A more complicated example that include manual data/training processing with Pytorch: [Transformers for Multi-Label Classification made simple](https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1)\n",
    "5. [Text Classification with tensorflow](https://github.com/huggingface/notebooks/blob/master/examples/text_classification-tf.ipynb): tensorflow example"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
