{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d9f6f3",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation (WSD)\n",
    "## Using BERT Masked Language Model (LM)\n",
    "This notebook explores the a part of the idea proposed by Ajit Rakasekharan in his blog post \n",
    "[Examining BERT raw embeddings.](https://towardsdatascience.com/examining-berts-raw-embeddings-fd905cb22df7) \n",
    "\n",
    "The idea is that examining the predictions of a masked language model for a masked ambiguous word can yield insights into the semantic meaning of the ambiguous word.\n",
    "\n",
    "We use the HuggingFace BERT for Masked LM with weights from a bert-base-cased pre-trained model for our experiment.\n",
    "\n",
    "We mask the ambiguous word (here we have used bank for our test) in sentences, and then send them through a BERT MLM model. Output is an array of logits for each position of the input sequence. So assuming a sentence with T tokens and a vocabulary size of V, the predictions of the MLM is (1, T, V) where 1 is the batch size (1 input sentence at a time in our experiment).\n",
    "\n",
    "In order to find the top k predictions, the logits for the masked position is softmaxed and the top k values chosen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef418b6",
   "metadata": {},
   "source": [
    "## Prepare your environment\n",
    "\n",
    "As always, we highly recommend that you install all packages with a virtual environment manager, like [venv](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) or [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html), to prevent version conflicts of different packages.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f05d0",
   "metadata": {},
   "source": [
    "### Masked LM Model and Tokenizer \n",
    "[tutorial](https://huggingface.co/docs/transformers/tasks/language_modeling)  \n",
    "Task is to predict words that are masked using BERT, so we will use BERTMaskedLM model and BERTTokenizer and use the pre-trained bert-base-uncased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c548bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ddf0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-cased', return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb19fe",
   "metadata": {},
   "source": [
    "We are going to use the pre-trained BERT language model in inference mode only.\n",
    "\n",
    "The tokenizer tokenizes the input sequence and pads it with the [CLS] and [SEP] tokens.\n",
    "\n",
    "The output produced by the model has two components, loss and logits. The logits component has shape (1, number_of_tokens, vocab_size) where the leading 1 represents the single input sentence.\n",
    "\n",
    "We will identify the logits corresponding to the position of our masked token, identify the top 5 vocabulary words predicted for that position, and return the softmax probabilities for each of the top 5 predicted words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88d39fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f0553c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'The', 'capital', 'of', 'France', 'is', '[MASK]', '.', '[SEP]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6db335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ -7.1545,  -6.9931,  -7.1826,  ...,  -5.9124,  -5.6733,  -5.9854],\n",
       "         [ -8.0190,  -8.1319,  -8.0509,  ...,  -6.5679,  -6.4058,  -6.8998],\n",
       "         [ -4.9772,  -6.1781,  -6.0669,  ...,  -5.6362,  -4.6603,  -5.1241],\n",
       "         ...,\n",
       "         [ -3.4420,  -3.2557,  -3.5733,  ...,  -2.4606,  -2.6495,  -3.1952],\n",
       "         [-10.5890, -10.4621, -11.7181,  ...,  -7.4646,  -9.9543,  -8.3927],\n",
       "         [-14.8900, -14.8873, -14.4569,  ..., -11.6588, -13.0151, -11.6073]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0433441f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mask_index(input_ids, tokenizer):\n",
    "  x = input_ids[0]\n",
    "  is_masked = torch.where(x == tokenizer.mask_token_id, x, 0)\n",
    "  mask_idx = torch.nonzero(is_masked)\n",
    "  return mask_idx.item()\n",
    "\n",
    "mask_idx = get_mask_index(inputs.input_ids, tokenizer)\n",
    "mask_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5192f8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paris', 44.46825087070465),\n",
       " ('Lyon', 9.396003931760788),\n",
       " ('Toulouse', 8.234527707099915),\n",
       " ('Lille', 7.515132427215576),\n",
       " ('Marseille', 5.692283064126968)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_k_predictions(pred_logits, mask_idx, top_k):\n",
    "  probs = torch.nn.functional.softmax(pred_logits[0, mask_idx, :], dim=-1)\n",
    "  top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "  top_k_pct_weights = [100 * x.item() for x in top_k_weights]\n",
    "  top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices)\n",
    "  return list(zip(top_k_tokens, top_k_pct_weights))\n",
    "\n",
    "get_top_k_predictions(outputs.logits, mask_idx, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b101bd3",
   "metadata": {},
   "source": [
    "### WSD Test Sentences\n",
    "We take our pair of sentences for disambiguating the word bank and mask them, and extract the top 20 predictions from the pre-trained BERT MLM model.\n",
    "\n",
    "As expected, the first set of predictions predominantly point to some sort of financial institution, whereas the second set of predictions predominantly point to some geographical formation around bodies of water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d178f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  \"Go to the [MASK] and deposit your pay check.\",\n",
    "  \"Jim and Janet went down to the river [MASK] to admire the swans.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f00b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(sentence, tokenizer, model):\n",
    "  inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "  outputs = model(**inputs)\n",
    "  mask_idx = get_mask_index(inputs.input_ids, tokenizer)\n",
    "  top_preds = get_top_k_predictions(outputs.logits, mask_idx, 20)\n",
    "  return top_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16805928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bank', 70.31400203704834),\n",
       " ('office', 10.280580818653107),\n",
       " ('register', 1.7451910302042961),\n",
       " ('store', 1.6284741461277008),\n",
       " ('bathroom', 0.9394760243594646),\n",
       " ('library', 0.893483217805624),\n",
       " ('desk', 0.8724337443709373),\n",
       " ('counter', 0.7977298460900784),\n",
       " ('hotel', 0.5163736641407013),\n",
       " ('lobby', 0.4956950433552265),\n",
       " ('kitchen', 0.36370735615491867),\n",
       " ('garage', 0.34799189306795597),\n",
       " ('door', 0.341272191144526),\n",
       " ('car', 0.3311359556391835),\n",
       " ('house', 0.26490497402846813),\n",
       " ('airport', 0.25470268446952105),\n",
       " ('elevator', 0.2491130493581295),\n",
       " ('back', 0.24807583540678024),\n",
       " ('computer', 0.24019514676183462),\n",
       " ('banks', 0.23491380270570517)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(sentences[0], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2c01c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('##bank', 32.60223567485809),\n",
       " ('below', 13.03189992904663),\n",
       " ('bank', 11.94087341427803),\n",
       " (',', 5.626500770449638),\n",
       " ('##boat', 3.1638897955417633),\n",
       " ('##front', 2.7332188561558723),\n",
       " ('basin', 1.6210518777370453),\n",
       " ('##bed', 1.2178423814475536),\n",
       " ('together', 1.1841695755720139),\n",
       " ('bed', 0.9657143615186214),\n",
       " ('again', 0.8369861170649529),\n",
       " ('deck', 0.8356181904673576),\n",
       " ('valley', 0.7271438371390104),\n",
       " ('mouth', 0.7227543275803328),\n",
       " ('boat', 0.7151042111217976),\n",
       " ('pier', 0.6493269931524992),\n",
       " ('house', 0.6301570683717728),\n",
       " ('banks', 0.5700557492673397),\n",
       " ('pool', 0.53457235917449),\n",
       " ('Thames', 0.4995575174689293)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(sentences[1], tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a23034",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "In this week's assignment, you are tasked with processing SemCor data and feed the data into BERT masked-LM. After that, use the predictions to find the most likely sense of the target word using WordNet similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5627e5",
   "metadata": {},
   "source": [
    "### Data Preprocessing \n",
    "You can find a sample of SemCor dataset [here](https://drive.google.com/file/d/1inmv3rUcGrtiS4VQwTMsT9HF-iL8jc5V/view?usp=sharing) and load the data using the following methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "099c2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "sents = []\n",
    "tokens = []\n",
    "wn_ids = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "with open('semcor.sample.jsonl') as f:\n",
    "  for line in f:\n",
    "    data = json.loads(line)\n",
    "    sents.append(data['sent'])\n",
    "    tokens.append(data['tokens'])\n",
    "    wn_ids.append(data['wnid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa495af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "implementation of georgia 's automobile title law was also recommended by the outgoing jury . \n",
      "['implementation', 'of', 'georgia', \"'s\", 'automobile', 'title', 'law', 'was', 'also', 'recommended', 'by', 'the', 'outgoing', 'jury', '.']\n",
      "['implementation%1:04:01::', 0, 'georgia%1:15:00::', 0, 'automobile%1:06:00::', 'title%1:10:04::', 'law%1:10:00::', 0, 'also%4:02:00::', 'recommend%2:32:01::', 0, 0, 'outgoing%3:00:00::', 'jury%1:14:00::', 0]\n"
     ]
    }
   ],
   "source": [
    "print(sents[10])\n",
    "print(tokens[10])\n",
    "print(wn_ids[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5cfee47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/bill/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/bill/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14229b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lemma('execution.n.06.implementation')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The WordNet ID can be converted to NLTK Lemma using the following function\n",
    "wn.lemma_from_key('implementation%1:04:01::')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac4dbe",
   "metadata": {},
   "source": [
    "### TODO \n",
    "Please implement a method to convert the data to BERT Masked-LM format and keep track of the headword. Store the data into the following lists\n",
    "\n",
    "headword[i] = 'implementation'  \n",
    "ground_truth[i] = 'implementation%1:04:01::'  \n",
    "sent[i] = \"[MASK] of georgia 's automobile title law was also recommended by the outgoing jury .\"  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14784f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the fulton_county_grand_jury said friday an investigation of atlanta \\'s recent primary_election produced \" no evidence \" that any irregularities took_place . '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac112c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'fulton',\n",
       " 'county',\n",
       " 'grand',\n",
       " 'jury',\n",
       " 'said',\n",
       " 'friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " 'atlanta',\n",
       " \"'s\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc1e4497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 'say%2:32:00::',\n",
       " 'friday%1:28:00::',\n",
       " 0,\n",
       " 'investigation%1:09:00::',\n",
       " 0,\n",
       " 'atlanta%1:15:00::',\n",
       " 0,\n",
       " 'recent%3:00:00:past:00',\n",
       " 'primary_election%1:04:00::',\n",
       " 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_ids[0][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daaf2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "headwords = []\n",
    "ground_truths = []\n",
    "for sent_idx in range(len(sents)):\n",
    "  for word_idx in range(len(tokens[sent_idx])):\n",
    "    try:\n",
    "      ground_truth = wn.lemma_from_key(wn_ids[sent_idx][word_idx])\n",
    "    except:\n",
    "      continue\n",
    "    if wn_ids[sent_idx][word_idx] == 0:\n",
    "      continue\n",
    "    headwords.append(tokens[sent_idx][word_idx])\n",
    "    ground_truths.append(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ded5e51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['said', 'friday', 'investigation', 'atlanta', 'primary']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca6946d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('state.v.01.say'),\n",
       " Lemma('friday.n.01.Friday'),\n",
       " Lemma('probe.n.01.investigation'),\n",
       " Lemma('atlanta.n.01.Atlanta'),\n",
       " Lemma('primary.n.01.primary_election')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c242ea",
   "metadata": {},
   "source": [
    "# Generate masked sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ea372f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01[MASK]56789'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_masked_sentence(sent, start_idx, end_idx):\n",
    "  left_sent = sent[:start_idx]\n",
    "  mid_sent = \"[MASK]\"\n",
    "  right_sent = sent[end_idx + 1:]\n",
    "  return left_sent + mid_sent + right_sent\n",
    "\n",
    "get_masked_sentence(\"0123456789\", 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e08538b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: I said what !he said err test -> error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I said what !he said err [MASK]']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sent_masked_sentences(sent, sent_tokens, sent_wn_ids):\n",
    "  masked_sentences = []\n",
    "  sent_char_idx = 0\n",
    "  for word_idx in range(len(sent_tokens)):\n",
    "    try:\n",
    "      ground_truth = wn.lemma_from_key(wn_ids[sent_idx][word_idx])\n",
    "    except:\n",
    "      continue\n",
    "    if sent_wn_ids[word_idx] == 0:\n",
    "      continue\n",
    "    word = sent_tokens[word_idx]\n",
    "    start_idx = sent.find(word, sent_char_idx)\n",
    "    if start_idx == -1:\n",
    "      print(\"ERROR: \" + sent + \" -> \" + word)\n",
    "    else:\n",
    "      end_idx = start_idx + len(word) - 1\n",
    "      masked_sentences.append(get_masked_sentence(sent, start_idx, end_idx))\n",
    "      sent_char_idx = end_idx + 1\n",
    "  return masked_sentences\n",
    "\n",
    "get_sent_masked_sentences(\n",
    "  \"I said what !he said err test\",\n",
    "  [\"I\", \"said\", \"what\", \"he\", \"said\", \"error\", \"test\"],\n",
    "  [0, 1, 0, 1, 1, 1, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "162db185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the fulton_county_grand_jury [MASK] friday an investigation of atlanta \\'s recent primary_election produced \" no evidence \" that any irregularities took_place . ',\n",
       " 'the fulton_county_grand_jury said [MASK] an investigation of atlanta \\'s recent primary_election produced \" no evidence \" that any irregularities took_place . ',\n",
       " 'the fulton_county_grand_jury said friday an [MASK] of atlanta \\'s recent primary_election produced \" no evidence \" that any irregularities took_place . ',\n",
       " 'the fulton_county_grand_jury said friday an investigation of [MASK] \\'s recent primary_election produced \" no evidence \" that any irregularities took_place . ',\n",
       " 'the fulton_county_grand_jury said friday an investigation of atlanta \\'s recent [MASK]_election produced \" no evidence \" that any irregularities took_place . ']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sents = []\n",
    "for sent_idx in range(len(sents)):\n",
    "  masked_sents.extend(get_sent_masked_sentences(sents[sent_idx], tokens[sent_idx], wn_ids[sent_idx]))\n",
    "\n",
    "masked_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a71abba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "971"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(masked_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e011450",
   "metadata": {},
   "source": [
    "#### Identify the top 5 predictions other than the headword using Masked-LM \n",
    "1. Use get_predictions to get the predicted words\n",
    "2. Use lemmatizer to lemmatize the prediction\n",
    "3. Remove headword\n",
    "4. Keep top 5 unique predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b94c14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['found', 'reported', ',', 'told', '_'],\n",
       " ['that', 'after', ',', 'in', 'during'],\n",
       " ['analysis', 'examination', 'audit', 'evaluation', 'inspection'],\n",
       " ['California', 'Obama', 'Alabama', 'Virginia', 'Arizona'],\n",
       " ['re', 'recall', 'by', 'municipal', 'mayor']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headwords_candidate_lemmas = []\n",
    "for headword_idx in range(len(headwords)):\n",
    "  predictions = get_predictions(masked_sents[headword_idx], tokenizer, model)\n",
    "  no_headword_predictions = [prediction for prediction in predictions if prediction[0] != headwords[headword_idx]]\n",
    "  headword_candidate_lemmas = [no_headword_predictions[idx][0] for idx in range(5)]\n",
    "  headwords_candidate_lemmas.append(headword_candidate_lemmas)\n",
    "\n",
    "headwords_candidate_lemmas[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b79a3c9",
   "metadata": {},
   "source": [
    "example:  \n",
    "candidate_lemmas = ['office', 'register', 'store', 'bathroom', 'library']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a7e8e",
   "metadata": {},
   "source": [
    "# Calculate sense cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "976b935a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('probe.n.01'), Synset('investigation.n.02')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_synsets = wn.synsets(\"investigation\")\n",
    "test_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71f969c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['analysis', 'examination', 'audit', 'evaluation', 'inspection']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_candidate_lemmas = headwords_candidate_lemmas[2]\n",
    "test_candidate_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba9b3202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headword_synset_cost(headword_synset, headword_candidate_lemmas):\n",
    "  cost = 0\n",
    "  for candidate_lemma in headword_candidate_lemmas:\n",
    "    for candidate_synset in wn.synsets(candidate_lemma):\n",
    "      cost += headword_synset.wup_similarity(candidate_synset)\n",
    "  return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c470a298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.133268030185516"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_headword_synset_cost(test_synsets[0], test_candidate_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81642e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.668906617977827"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_headword_synset_cost(test_synsets[1], test_candidate_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f18328a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('investigation.n.02')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_headword_predicted_synset(headword, headword_candidate_lemmas):\n",
    "  max_cost = -math.inf\n",
    "  predicted_synset = None\n",
    "  for headword_synset in wn.synsets(headword):\n",
    "    cost = get_headword_synset_cost(headword_synset, headword_candidate_lemmas)\n",
    "    if cost > max_cost:\n",
    "      cost = max_cost\n",
    "      predicted_synset = headword_synset\n",
    "  return predicted_synset\n",
    "\n",
    "get_headword_predicted_synset(\"investigation\", test_candidate_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bf3a95",
   "metadata": {},
   "source": [
    "Identify the most similar sense of headword with relation to the 5 unique candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30c0dab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('aforesaid.s.01'),\n",
       " Synset('friday.n.01'),\n",
       " Synset('investigation.n.02'),\n",
       " Synset('atlanta.n.02'),\n",
       " Synset('basal.s.03')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_synsets = []\n",
    "for headword_idx in range(len(headwords)):\n",
    "  predicted_synset = get_headword_predicted_synset(headwords[headword_idx],\n",
    "                                                   headwords_candidate_lemmas[headword_idx])\n",
    "  predicted_synsets.append(predicted_synset)\n",
    "\n",
    "predicted_synsets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3cf473",
   "metadata": {},
   "source": [
    "For evaluation purpose, for i = 50, please run the process and print out the following:  \n",
    "1. word[50]\n",
    "2. ground_truth[50] (in synset or lemma)\n",
    "3. sent[50]\n",
    "4. candidate_lemmas\n",
    "5. predicted_sense (in synset or lemma)    \n",
    "\n",
    "Also, please print out the accuracy of the process over our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5920742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find\n",
      "Lemma('rule.v.04.find')\n",
      "he will be succeeded by rob_ledford of gainesville , who has been an assistant more than three years . \n",
      "['note', 'show', 'notice', 'believe', 'say']\n",
      "Synset('find_oneself.v.01')\n"
     ]
    }
   ],
   "source": [
    "print(headwords[50])\n",
    "print(ground_truths[50])\n",
    "print(sents[50])\n",
    "print(headwords_candidate_lemmas[50])\n",
    "print(predicted_synsets[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0367a68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lemma('rule.v.04.find')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "718a57d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8317757009345794"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_nums = 0\n",
    "total_nums = len(headwords)\n",
    "for headword_idx in range(len(headwords)):\n",
    "  if len(wn.synsets(headwords[headword_idx])) == 0:\n",
    "    total_nums -= 1\n",
    "    continue\n",
    "  if ground_truths[headword_idx] in predicted_synsets[headword_idx].lemmas():\n",
    "    correct_nums += 1\n",
    "correct_nums / total_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1225036f",
   "metadata": {},
   "source": [
    "## TA's Note\n",
    "\n",
    "Congratulations, you made it to the end of the tutorial! Make sure you make an appointment to show your work and turn in your finished assignment before next week's lesson. We will ask you to run your code, so double check that everything is working and that your model is saved. Don't worry if you didn't pass the evaluation requirements, you'll still get partial points for trying."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
